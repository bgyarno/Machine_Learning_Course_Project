---
title: "Course Project"
author: "Brian Yarno"
date: "July 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```
## Introduction
This study analyzes sensor data taken from subjects performing barbell lifts in 5 different ways, with one being the "correct" way and the remaining four considered incorrect form.  The goal of the study is to develop a machine learning model that accurately predicts which of the five classifications of lifts were performed based on the sensor readings.  Dimensionality reduction was performed via Principal Component Analysis.  Several models were evaluated, including models that combined predictors, and two different methods were used in handing missing data, in an effort to identify the model with the maximum accuracy measurement.

## Data Preparation
Aside from missing values, which will be handled in a later section, this dataset is mostly clean.  The only transformation we perform is to convert the cvtd_timestamp variable to a POSIXct data type.

```{r dataprep}
library(caret)

if(!dir.exists('./data')) {dir.create('./data')}
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', './data/training.csv')
download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', './data/testing.csv')

training <- read.csv('./data/training.csv')
testing <- read.csv('./data/testing.csv')

## format date string as POSIXct in both datasets
training$cvtd_timestamp <- as.POSIXct(training$cvtd_timestamp, format = '%m/%d/%Y %H:%M')
testing$cvtd_timestamp <- as.POSIXct(testing$cvtd_timestamp, format = '%m/%d/%Y %H:%M')
```
### Partition data in to training and validation sets
The training data provided will be split into a training and validation dataset, with validation data being used to evaluate the performance of each model.  We first check the distribution of the outcome classes.  If the outcomes are overwhelmingly skewed to one class, we may need to re-balance the training data to ensure predictions are not biased to the dominant class.  Fortunately our training data is relatively well balanced.  We see that there is a higher percentage of records in the "A" class, however this is not unbalanced enough to warrant adjusting the training data.
```{r exploration}
library(ggplot2)
ggplot(training, aes(classe)) + geom_bar(aes(y = (..count..)/sum(..count..))) + labs(y = "percent of total observations")
```

We next partition the training data in to a training and validation set.  The models will be trained on the training data, and evaluated against the validation data to determine model accuracy.  The test set that is provided is used as the final evaluation outside of this paper, but is included here so that all dataset transformations are consistent across the three datasets.  We set the training/validation split at 60/40.

```{r partition}
set.seed(12345)
partition <- createDataPartition(training$classe, p = 0.6, list = FALSE)
train_part <- training[partition,]
valid_part <- training[-partition,]
```

### Addressing Missing Data
We see that about 64% of the values in the training dataset are missing.  We also see that variables either have no missing values, or are almost entirely missing.  We will take two approaches to handle missing data and fit models to datasets for both methods.  The first method will be to remove the variables with high levels of missing data from the dataset entirely.  The second method will be to impute the missing values using kNN imputation.  In order to achieve the latter we set all data observations with null values to NA.  When we impute the missing values, we notice that a handful of variables have a zero variance, so we remove these variables from the dataset.  In addition, we will remove the first 7 variables from both datasets.  These are identifying variables such as subject name, timestamp, etc. We determine that variables such as this would provide no value in predicting new observations.

```{r missing}
## Make empty values NA for both training and testing datasets
training_NA <- as.data.frame(sapply(train_part[8:159], function(x) ifelse(x == "", NA, x)))
valid_NA <- as.data.frame(sapply(valid_part[8:159], function(x) ifelse(x == "", NA, x)))
testing_NA <- as.data.frame(sapply(testing[8:159], function(x) ifelse(x == "", NA, x)))

## check percent missing values
mean(is.na(training_NA))
## check distribution of missing value percentages
unique(apply(training_NA, 2, function(x) mean(is.na(x))))

```

### Method 1:  remove variables with missing values
The script below removes all variables that have missing data from the training, validation, and testing datasets
```{r remove missing}
## create training and testing dataset without variables with high levels of missing values
missing <- apply(training_NA, 2, function(x) mean(is.na(x)) > 0)

training_nomiss <- training_NA[, names(training_NA) %in% names(missing[missing == TRUE]) == FALSE]
valid_nomiss <- valid_NA[, names(valid_NA) %in% names(missing[missing == TRUE]) == FALSE]
testing_nomiss <- testing_NA[, names(testing_NA) %in% names(missing[missing == TRUE]) == FALSE]
```

### Method 2:  impute values for missing data
The script below replaces missing values with estimated values using kNN imputation.  The imputation algorithm is trained on the training dataset, then applied to the training, validation, and testing datasets to replace missing values.  The imputation algorithm identifies six variables that have a zero variance.  These variables are removed from the dataset.
```{r impute}
## impute values for both datasets
imputed <- preProcess(training_NA, method = "knnImpute")

train_impute <- predict(imputed, training_NA)
valid_impute <- predict(imputed, valid_NA)
test_impute <- predict(imputed, testing_NA)

## remove variables with near zero variance in the test data
imputed_zero <- nearZeroVar(train_impute)
train_impute <- train_impute[, -(imputed_zero)]
valid_impute <- valid_impute[, -(imputed_zero)]
test_impute <- test_impute[, -(imputed_zero)]

```

### Principal Component Analysis
With the missing values addressed, we now have two training datasets, one with 145 variables, and one with 52.  We will perform Principal Component Analysis on both datasets in an effort to reduce the overall number of predictors.  We choose .95 as a threshold for the variability represented on the components returned.  This leaves us with 25 and 44 components returned for the dataset with variables removed, and the dataset with imputed values, respectively.  This PCA model is also applied to the validation and testing data.
```{r pca}
## perform PCA on dataset where variables removed
nomiss_pca <- preProcess(training_nomiss, method = "pca", thresh = .95)
nomiss_pca

## perform PCA on dataset with imputed values, removing those with near zero variance
imputed_pca <- preProcess(train_impute, method = "pca", thresh = .95)
imputed_pca

## create training and testing datasets for both methods with principal components
train_nomissPCA <- predict(nomiss_pca, training_nomiss)
valid_nomissPCA <- predict(nomiss_pca, valid_nomiss)
test_nomissPCA <- predict(nomiss_pca, testing_nomiss)

train_imputePCA <- predict(imputed_pca, train_impute)
valid_imputePCA <- predict(imputed_pca, valid_impute)
test_imputePCA <- predict(imputed_pca, test_impute)


```

## Model Training and Cross Validation Method
Next we train a series of models on the training datasets and evaluate them against the validation datasets. We see that the Random Forest models are the top performing in both versions of the data with accuracy in the 90% range when tested on the out of sample validation dataset

The cross validation method for all models is a five-fold cross-validation, meaning the training data is split into five partitions, the model is trained on four of the partitions, and tested on the fifth.  This process is repeated for each combination of the five folds.  In addition, all models will be applied to the hold out validation dataset.  Model accuracy on this dataset will be used to select the best model

```{r train, results='hide'}
library(parallel)
library(doParallel)
library(randomForest)
library(gbm)
library(plyr)
library(MASS)
library(nnet)

## set parameters for training control and allowing parallel processing
set.seed(12345)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

## train models with missing variables removed
nomiss_rf <- train(train_nomissPCA, train_part[,160], method = "rf", trControl = fitControl)
nomiss_boost <- train(train_nomissPCA, train_part[,160], method = "gbm", trControl = fitControl)
nomiss_lda <- train(train_nomissPCA, train_part[,160], method = "lda", trControl = fitControl)
nomiss_nnet <- train(train_nomissPCA, train_part[,160], method = "nnet", trControl = fitControl)

## train models with imputed values
imputed_rf <- train(train_imputePCA, train_part[,160], method = "rf", trControl = fitControl)
imputed_boost <- train(train_imputePCA, train_part[,160], method = "gbm", trControl = fitControl)
imputed_lda <- train(train_imputePCA, train_part[,160], method = "lda", trControl = fitControl)
imputed_nnet <- train(train_imputePCA, train_part[,160], method = "nnet", trControl = fitControl)

## Apply trained models to the respective test data
valid_nomiss_rf <- predict(nomiss_rf, valid_nomissPCA)
valid_nomiss_boost <- predict(nomiss_boost, valid_nomissPCA)
valid_nomiss_lda <- predict(nomiss_lda, valid_nomissPCA)
valid_nomiss_nnet <- predict(nomiss_lda, valid_nomissPCA)

valid_imputed_rf <- predict(imputed_rf, valid_imputePCA)
valid_imputed_boost <- predict(imputed_boost, valid_imputePCA)
valid_imputed_lda <- predict(imputed_lda, valid_imputePCA)
valid_imputed_nnet <- predict(imputed_nnet, valid_imputePCA)

## Evaluate model performance
nomiss_rf_matrix <- confusionMatrix(valid_nomiss_rf, valid_part[,160])
nomiss_boost_matrix <- confusionMatrix(valid_nomiss_boost, valid_part[,160])
nomiss_lda_matrix <- confusionMatrix(valid_nomiss_lda, valid_part[,160])
nomiss_nnet_matrix <- confusionMatrix(valid_nomiss_nnet, valid_part[,160])

imputed_rf_matrix <- confusionMatrix(valid_imputed_rf, valid_part[,160])
imputed_boost_matrix <- confusionMatrix(valid_imputed_boost, valid_part[,160])
imputed_lda_matrix <- confusionMatrix(valid_imputed_lda, valid_part[,160])
imputed_nnet_matrix <- confusionMatrix(valid_imputed_nnet, valid_part[,160])

## compare model accuracy
models <- c("Random Forest (nomissing)", "Random Forest (imputed)", "GBM (nomissing)", "GBM (imputed)", "LDA (nomissing)", "LDA (imputed)", "Neural Network (nomiss)", "Neural Network (imputed)")
accuracy <- c(nomiss_rf_matrix$overall[1], imputed_rf_matrix$overall[1], nomiss_boost_matrix$overall[1], imputed_boost_matrix$overall[1], nomiss_lda_matrix$overall[1], imputed_lda_matrix$overall[1], nomiss_nnet_matrix$overall[1], imputed_nnet_matrix$overall[1] )
```

```{r output}
## print dataframe comparing model performance on validation data
model_stats <- data.frame(models, accuracy)
model_stats

```
## Combining Predictors
In an attempt to improve on the accuracy of our top performing model, we will combine predictors from the top five performing models via two methods, a simple majority vote, and a blended model using each model's predictions.

We see that the simple majority vote model has an accuracy equal to the best random forest model, while the blended model performs much worse than the best models evaluated

```{r ensemble}
library(mgcv)

train_combo_df <- data.frame(predict(nomiss_rf, train_nomissPCA), predict(imputed_rf, train_imputePCA), predict(imputed_boost, train_imputePCA), predict(nomiss_boost, train_nomissPCA), predict(imputed_nnet, train_imputePCA))
names(train_combo_df) <- c("rf1", "rf2", "boost1", "boost2", "nnet")

valid_combo_df <- data.frame(predict(nomiss_rf, valid_nomissPCA), predict(imputed_rf, valid_imputePCA), predict(imputed_boost, valid_imputePCA), predict(nomiss_boost, valid_nomissPCA), predict(imputed_nnet, valid_imputePCA))
names(valid_combo_df) <- c("rf1", "rf2", "boost1", "boost2", "nnet")

## create function that returns the majority vote.  In the case of a tie, returns the prediction of the best performing model
majority <- function(x){
    A <- sum(x == "A")
    B <- sum(x == "B")
    C <- sum(x == "C")
    D <- sum(x == "D")
    E <- sum(x == "E")
    
    results <- data.frame( classes = c("A", "B", "C", "D", "E"), counts = c(A, B, C, D, E))
    max <- results[results$counts == max(results$counts),]
    if(length(max) == 1){
        winner <- max
        }
    else winner <- x[1]
    
    winner
}

## predict classes based on majority vote and evaluate accuracy
vote <- apply(valid_combo_df, 1, majority)
vote_matrix <- confusionMatrix(vote, valid_part[,160])
vote_matrix$overall[1]

## train stacked model of five best performing models
stacked <- train(train_combo_df, train_part[,160], method = "gam", trControl = fitControl)
valid_stacked <- predict(stacked, valid_combo_df)
stacked_matrix <- confusionMatrix(valid_stacked, valid_part[,160])
stacked_matrix$overall[1]

```

## Conclusion
While the random forest model using the dataset with missing values removed and the majority vote model returned the same accuracy scores, the random forest model was selected as the best model and applied to the testing data.  This model correctly predicted 19 of 20 observations on the test dataset, for an accuracy of 95%, consistent with model's performance on the validation data.